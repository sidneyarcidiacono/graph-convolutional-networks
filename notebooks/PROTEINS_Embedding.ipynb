{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PROTEINS_Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lqL-LQstoi0"
      },
      "source": [
        "# Graph Neural Network Classification on the PROTEINS Dataset\n",
        "\n",
        "For the first approach, I'm going to use [Spektral](https://graphneural.network/getting-started/) for Python to build my GCN layer and then perform our classification. \n",
        "\n",
        "Spektral is a library for Python for Graph Neural Networks, built on Tensorflow and Keras. \n",
        "\n",
        "Our second experiment will be built with PyTorch Geometric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_884Zbltkwo"
      },
      "source": [
        "# !pip install spektral"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw3cynAtwQOE",
        "outputId": "34603f28-a842-4d90-eb99-5b5256ccfa0f"
      },
      "source": [
        "# Reading in the PROTEINS dataset\n",
        "from spektral.datasets import TUDataset\n",
        "\n",
        "data = TUDataset('PROTEINS')\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading PROTEINS dataset.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████| 447k/447k [00:00<00:00, 854kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully loaded PROTEINS.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TUDataset(n_graphs=1113)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AZCWahwwj7A"
      },
      "source": [
        "# Since we want to utilize the Spektral GCN layer, we want to follow the original paper for this method and perform some preprocessing:\n",
        "from spektral.transforms import GCNFilter\n",
        "\n",
        "data.apply(GCNFilter())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDNI3sTt5mJQ"
      },
      "source": [
        "# Split our train and test data. This just splits based on the first 80%/second 20% which isn't entirely ideal, so we'll shuffle the data first.\n",
        "import numpy as np\n",
        "\n",
        "np.random.shuffle(data)\n",
        "split = int(0.8 * len(data))\n",
        "data_train, data_test = data[:split], data[split:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwvnn79Ewm7M"
      },
      "source": [
        "# Spektral is built on top of Keras, so we can use the Keras functional API to build a model that first embeds,\n",
        "# then sums the nodes together (global pooling), then classifies the result with a dense softmax layer\n",
        "\n",
        "# First, let's import the necessary layers:\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from spektral.layers import GCNConv, GlobalSumPool"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrTSQXkZyW78"
      },
      "source": [
        "# Now, we can use model subclassing to define our model:\n",
        "\n",
        "class ProteinsGNN(Model):\n",
        "  \n",
        "  def __init__(self, n_hidden, n_labels):\n",
        "    super().__init__()\n",
        "    # Define our GCN layer with our n_hidden layers\n",
        "    self.graph_conv = GCNConv(n_hidden)\n",
        "    # Define our global pooling layer\n",
        "    self.pool = GlobalSumPool()\n",
        "    # Define our dropout layer, initialize dropout freq. to .5 (50%)\n",
        "    self.dropout = Dropout(0.5)\n",
        "    # Define our Dense layer, with softmax activation function\n",
        "    self.dense = Dense(n_labels, 'softmax')\n",
        "\n",
        "  # Define class method to call model on input\n",
        "  def call(self, inputs):\n",
        "    out = self.graph_conv(inputs)\n",
        "    out = self.dropout(out)\n",
        "    out = self.pool(out)\n",
        "    out = self.dense(out)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CWW1urKzRrU"
      },
      "source": [
        "# Instantiate our model for training\n",
        "model = ProteinsGNN(32, data.n_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG2YY6CMzf3I"
      },
      "source": [
        "# Compile model with our optimizer (adam) and loss function\n",
        "model.compile('adam', 'categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw1hlui8zpYg"
      },
      "source": [
        "# Here's the trick - we can't just call Keras' fit() method on this model.\n",
        "# Instead, we have to use Loaders, which Spektral walks us through. Loaders create mini-batches by iterating over the graph\n",
        "# Since we're using Spektral for an experiment, for our first trial we'll use the recommended loader in the getting started tutorial\n",
        "\n",
        "# TODO: read up on modes and try other loaders later\n",
        "from spektral.data import BatchLoader\n",
        "\n",
        "loader = BatchLoader(data_train, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmJ70FZy0Jgw",
        "outputId": "51248f2c-ac4b-44b7-b141-c8301a2b81f3"
      },
      "source": [
        "# Now we can train! We don't need to specify a batch size, since our loader is basically a generator\n",
        "# But we do need to specify the steps_per_epoch parameter\n",
        "\n",
        "model.fit(loader.load(), steps_per_epoch=loader.steps_per_epoch, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "28/28 [==============================] - 2s 29ms/step - loss: 15.5761\n",
            "Epoch 2/10\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 5.7475\n",
            "Epoch 3/10\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 5.5035\n",
            "Epoch 4/10\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 4.6222\n",
            "Epoch 5/10\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 4.6596\n",
            "Epoch 6/10\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 5.2084\n",
            "Epoch 7/10\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 4.8452\n",
            "Epoch 8/10\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 5.2131\n",
            "Epoch 9/10\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 5.6248\n",
            "Epoch 10/10\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 4.6807\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3210617f50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GOzdAb01Ei4"
      },
      "source": [
        "# To evaluate, let's instantiate another loader to test\n",
        "\n",
        "test_loader = BatchLoader(data_test, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wZS1SEK6H4o",
        "outputId": "667d70a5-b5fa-4897-c742-5df8621ae6b7"
      },
      "source": [
        "# And feed it to our model by calling .load()\n",
        "\n",
        "loss = model.evaluate(loader.load(), steps=loader.steps_per_epoch)\n",
        "\n",
        "print('Test loss: {}'.format(loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28/28 [==============================] - 1s 15ms/step - loss: 3.5004\n",
            "Test loss: 3.500354528427124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Lud8oKFzGk"
      },
      "source": [
        "## PyTorch Geometric GCN\n",
        "\n",
        "Pytorch Geometric provides [GCN layers](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html) based on Kipf & Welling's original paper: [\"Semi-Supervised Classification with Graph Convolutional Networks\"](https://arxiv.org/abs/1609.02907) on which I've based the bulk of my research and write-ups.\n",
        "\n",
        "While my original goal was to use my [original experiment](https://colab.research.google.com/drive/1NUQgUdrdvIddewdQyGEpas_mPaFzC8-e?usp=sharing) (based off of [this](https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b) resource) to build this from scratch, I ran into difficulties trying to embed and classify such a large dataset, specifically with Colab RAM allowances.\n",
        "\n",
        "For this reason, I sought out different methods and found that this problem had already been solved, and for purposes of time and demonstration chose to delve into Pytorch Geometric rather than invent the wheel. \n",
        "\n",
        "In order to successfully learn to implement this approach with this library, I relied on the Pytorch Geometric [documentation](https://pytorch-geometric.readthedocs.io/en/latest/index.html) as well as [this notebook](https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing) written by matthias.fey@tu-dortmund.de. \n",
        "\n",
        "I would like to extend thanks and all due credit to these authors, as this work and research would not be possible without them. Further credits and citations can be found in the [README](https://github.com/sidneyarcidiacono/graph-convolutional-networks) of this repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbFimfvQTMuz"
      },
      "source": [
        "# Install required packages.\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-geometric"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXT8XfFcQZU5",
        "outputId": "3d7aa25b-c180-4949-d2c7-50f1b418243c"
      },
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "dataset = TUDataset(root='data/TUDataset', name='PROTEINS')\n",
        "\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('====================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0]  # Get the first graph object.\n",
        "\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('====================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
        "print(f'Contains self-loops: {data.contains_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset: PROTEINS(1113):\n",
            "====================\n",
            "Number of graphs: 1113\n",
            "Number of features: 3\n",
            "Number of classes: 2\n",
            "\n",
            "Dataset: PROTEINS(1113):\n",
            "====================\n",
            "Number of graphs: 1113\n",
            "Number of features: 3\n",
            "Number of edges: 162\n",
            "Average node degree: 3.86\n",
            "Contains isolated nodes: False\n",
            "Contains self-loops: False\n",
            "Is undirected: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-pZeqpXQcIt",
        "outputId": "8ce52c8b-131c-4990-ea84-13b3474676f1"
      },
      "source": [
        "torch.manual_seed(12345)\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "train_dataset = dataset[:150]\n",
        "test_dataset = dataset[150:]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training graphs: 150\n",
            "Number of test graphs: 963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ERyFSsCQAcQ",
        "outputId": "f7b6d779-9af2-4c95-f266-38a93976ac44"
      },
      "source": [
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "for step, data in enumerate(train_loader):\n",
        "    print(f'Step {step + 1}:')\n",
        "    print('=======')\n",
        "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
        "    print(data)\n",
        "    print()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "Batch(batch=[2534], edge_index=[2, 9562], ptr=[65], x=[2534, 3], y=[64])\n",
            "\n",
            "Step 2:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "Batch(batch=[2599], edge_index=[2, 9512], ptr=[65], x=[2599, 3], y=[64])\n",
            "\n",
            "Step 3:\n",
            "=======\n",
            "Number of graphs in the current batch: 22\n",
            "Batch(batch=[604], edge_index=[2, 2288], ptr=[23], x=[604, 3], y=[22])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Buu47UjnOJtW",
        "outputId": "35d13bcc-865f-4634-d5c9-de0c4c7d7e1d"
      },
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "model = GCN(hidden_channels=64)\n",
        "print(model)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(3, 64)\n",
            "  (conv2): GCNConv(64, 64)\n",
            "  (conv3): GCNConv(64, 64)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpON7LPyOYJB",
        "outputId": "50f6992c-ef6f-4d7a-88ac-7a4f12472997"
      },
      "source": [
        "model = GCN(hidden_channels=64)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "      out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
        "      loss = criterion(out, data.y)  # Compute the loss.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "def test(loader):\n",
        "  model.eval()\n",
        "\n",
        "  correct = 0\n",
        "  for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "      out = model(data.x, data.edge_index, data.batch)  \n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "  return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
        "\n",
        "\n",
        "for epoch in range(1, 201):\n",
        "    train()\n",
        "    train_acc = test(train_loader)\n",
        "    test_acc = test(test_loader)\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001, Train Acc: 0.6000, Test Acc: 0.5950\n",
            "Epoch: 002, Train Acc: 0.6000, Test Acc: 0.5950\n",
            "Epoch: 003, Train Acc: 0.6667, Test Acc: 0.6303\n",
            "Epoch: 004, Train Acc: 0.6000, Test Acc: 0.5950\n",
            "Epoch: 005, Train Acc: 0.6000, Test Acc: 0.5950\n",
            "Epoch: 006, Train Acc: 0.6267, Test Acc: 0.6085\n",
            "Epoch: 007, Train Acc: 0.7000, Test Acc: 0.6677\n",
            "Epoch: 008, Train Acc: 0.7000, Test Acc: 0.6739\n",
            "Epoch: 009, Train Acc: 0.7067, Test Acc: 0.6771\n",
            "Epoch: 010, Train Acc: 0.7267, Test Acc: 0.6771\n",
            "Epoch: 011, Train Acc: 0.7067, Test Acc: 0.6739\n",
            "Epoch: 012, Train Acc: 0.7067, Test Acc: 0.6771\n",
            "Epoch: 013, Train Acc: 0.7133, Test Acc: 0.6791\n",
            "Epoch: 014, Train Acc: 0.7200, Test Acc: 0.6833\n",
            "Epoch: 015, Train Acc: 0.7333, Test Acc: 0.6781\n",
            "Epoch: 016, Train Acc: 0.6800, Test Acc: 0.6355\n",
            "Epoch: 017, Train Acc: 0.7533, Test Acc: 0.6854\n",
            "Epoch: 018, Train Acc: 0.7000, Test Acc: 0.6521\n",
            "Epoch: 019, Train Acc: 0.6867, Test Acc: 0.6646\n",
            "Epoch: 020, Train Acc: 0.6733, Test Acc: 0.6490\n",
            "Epoch: 021, Train Acc: 0.7200, Test Acc: 0.6708\n",
            "Epoch: 022, Train Acc: 0.7267, Test Acc: 0.6750\n",
            "Epoch: 023, Train Acc: 0.7200, Test Acc: 0.6708\n",
            "Epoch: 024, Train Acc: 0.7000, Test Acc: 0.6459\n",
            "Epoch: 025, Train Acc: 0.7000, Test Acc: 0.6480\n",
            "Epoch: 026, Train Acc: 0.7133, Test Acc: 0.6719\n",
            "Epoch: 027, Train Acc: 0.6400, Test Acc: 0.6366\n",
            "Epoch: 028, Train Acc: 0.7600, Test Acc: 0.6822\n",
            "Epoch: 029, Train Acc: 0.7000, Test Acc: 0.6449\n",
            "Epoch: 030, Train Acc: 0.7333, Test Acc: 0.6750\n",
            "Epoch: 031, Train Acc: 0.7133, Test Acc: 0.6636\n",
            "Epoch: 032, Train Acc: 0.7133, Test Acc: 0.6636\n",
            "Epoch: 033, Train Acc: 0.7133, Test Acc: 0.6677\n",
            "Epoch: 034, Train Acc: 0.7200, Test Acc: 0.6864\n",
            "Epoch: 035, Train Acc: 0.7400, Test Acc: 0.6833\n",
            "Epoch: 036, Train Acc: 0.7400, Test Acc: 0.6760\n",
            "Epoch: 037, Train Acc: 0.7000, Test Acc: 0.6677\n",
            "Epoch: 038, Train Acc: 0.7400, Test Acc: 0.6729\n",
            "Epoch: 039, Train Acc: 0.7467, Test Acc: 0.6739\n",
            "Epoch: 040, Train Acc: 0.7533, Test Acc: 0.6750\n",
            "Epoch: 041, Train Acc: 0.7733, Test Acc: 0.6906\n",
            "Epoch: 042, Train Acc: 0.7400, Test Acc: 0.6822\n",
            "Epoch: 043, Train Acc: 0.7333, Test Acc: 0.6833\n",
            "Epoch: 044, Train Acc: 0.7600, Test Acc: 0.6698\n",
            "Epoch: 045, Train Acc: 0.7600, Test Acc: 0.6636\n",
            "Epoch: 046, Train Acc: 0.7400, Test Acc: 0.6968\n",
            "Epoch: 047, Train Acc: 0.7267, Test Acc: 0.6656\n",
            "Epoch: 048, Train Acc: 0.7400, Test Acc: 0.6698\n",
            "Epoch: 049, Train Acc: 0.7333, Test Acc: 0.6791\n",
            "Epoch: 050, Train Acc: 0.7667, Test Acc: 0.6615\n",
            "Epoch: 051, Train Acc: 0.7400, Test Acc: 0.6926\n",
            "Epoch: 052, Train Acc: 0.7000, Test Acc: 0.6501\n",
            "Epoch: 053, Train Acc: 0.7400, Test Acc: 0.6708\n",
            "Epoch: 054, Train Acc: 0.7667, Test Acc: 0.6584\n",
            "Epoch: 055, Train Acc: 0.7267, Test Acc: 0.6480\n",
            "Epoch: 056, Train Acc: 0.7400, Test Acc: 0.6937\n",
            "Epoch: 057, Train Acc: 0.7067, Test Acc: 0.6563\n",
            "Epoch: 058, Train Acc: 0.6867, Test Acc: 0.6334\n",
            "Epoch: 059, Train Acc: 0.7067, Test Acc: 0.6376\n",
            "Epoch: 060, Train Acc: 0.7267, Test Acc: 0.6708\n",
            "Epoch: 061, Train Acc: 0.7600, Test Acc: 0.7051\n",
            "Epoch: 062, Train Acc: 0.7467, Test Acc: 0.6906\n",
            "Epoch: 063, Train Acc: 0.7333, Test Acc: 0.6729\n",
            "Epoch: 064, Train Acc: 0.7333, Test Acc: 0.6812\n",
            "Epoch: 065, Train Acc: 0.7533, Test Acc: 0.6864\n",
            "Epoch: 066, Train Acc: 0.7600, Test Acc: 0.6916\n",
            "Epoch: 067, Train Acc: 0.7600, Test Acc: 0.6947\n",
            "Epoch: 068, Train Acc: 0.7533, Test Acc: 0.6926\n",
            "Epoch: 069, Train Acc: 0.7467, Test Acc: 0.6791\n",
            "Epoch: 070, Train Acc: 0.7400, Test Acc: 0.6802\n",
            "Epoch: 071, Train Acc: 0.7333, Test Acc: 0.6791\n",
            "Epoch: 072, Train Acc: 0.7533, Test Acc: 0.6937\n",
            "Epoch: 073, Train Acc: 0.7667, Test Acc: 0.6989\n",
            "Epoch: 074, Train Acc: 0.7533, Test Acc: 0.6833\n",
            "Epoch: 075, Train Acc: 0.7600, Test Acc: 0.6729\n",
            "Epoch: 076, Train Acc: 0.7667, Test Acc: 0.7082\n",
            "Epoch: 077, Train Acc: 0.7667, Test Acc: 0.6999\n",
            "Epoch: 078, Train Acc: 0.7267, Test Acc: 0.6854\n",
            "Epoch: 079, Train Acc: 0.7400, Test Acc: 0.6771\n",
            "Epoch: 080, Train Acc: 0.7533, Test Acc: 0.6895\n",
            "Epoch: 081, Train Acc: 0.7800, Test Acc: 0.6615\n",
            "Epoch: 082, Train Acc: 0.7867, Test Acc: 0.6708\n",
            "Epoch: 083, Train Acc: 0.7533, Test Acc: 0.6812\n",
            "Epoch: 084, Train Acc: 0.7600, Test Acc: 0.6812\n",
            "Epoch: 085, Train Acc: 0.7533, Test Acc: 0.6729\n",
            "Epoch: 086, Train Acc: 0.7600, Test Acc: 0.6656\n",
            "Epoch: 087, Train Acc: 0.7600, Test Acc: 0.6864\n",
            "Epoch: 088, Train Acc: 0.7733, Test Acc: 0.6957\n",
            "Epoch: 089, Train Acc: 0.7533, Test Acc: 0.6739\n",
            "Epoch: 090, Train Acc: 0.6733, Test Acc: 0.6469\n",
            "Epoch: 091, Train Acc: 0.7267, Test Acc: 0.6698\n",
            "Epoch: 092, Train Acc: 0.7467, Test Acc: 0.6552\n",
            "Epoch: 093, Train Acc: 0.7800, Test Acc: 0.6667\n",
            "Epoch: 094, Train Acc: 0.6800, Test Acc: 0.6521\n",
            "Epoch: 095, Train Acc: 0.6800, Test Acc: 0.6501\n",
            "Epoch: 096, Train Acc: 0.7600, Test Acc: 0.6636\n",
            "Epoch: 097, Train Acc: 0.7533, Test Acc: 0.6490\n",
            "Epoch: 098, Train Acc: 0.7000, Test Acc: 0.6397\n",
            "Epoch: 099, Train Acc: 0.7200, Test Acc: 0.6511\n",
            "Epoch: 100, Train Acc: 0.7600, Test Acc: 0.6916\n",
            "Epoch: 101, Train Acc: 0.7733, Test Acc: 0.6802\n",
            "Epoch: 102, Train Acc: 0.7667, Test Acc: 0.6604\n",
            "Epoch: 103, Train Acc: 0.7467, Test Acc: 0.6802\n",
            "Epoch: 104, Train Acc: 0.7400, Test Acc: 0.6916\n",
            "Epoch: 105, Train Acc: 0.7600, Test Acc: 0.6874\n",
            "Epoch: 106, Train Acc: 0.7600, Test Acc: 0.6760\n",
            "Epoch: 107, Train Acc: 0.7533, Test Acc: 0.6947\n",
            "Epoch: 108, Train Acc: 0.7533, Test Acc: 0.6916\n",
            "Epoch: 109, Train Acc: 0.7467, Test Acc: 0.6802\n",
            "Epoch: 110, Train Acc: 0.7733, Test Acc: 0.6677\n",
            "Epoch: 111, Train Acc: 0.7667, Test Acc: 0.6739\n",
            "Epoch: 112, Train Acc: 0.7400, Test Acc: 0.6916\n",
            "Epoch: 113, Train Acc: 0.7733, Test Acc: 0.7040\n",
            "Epoch: 114, Train Acc: 0.7733, Test Acc: 0.6822\n",
            "Epoch: 115, Train Acc: 0.7533, Test Acc: 0.6739\n",
            "Epoch: 116, Train Acc: 0.7333, Test Acc: 0.6791\n",
            "Epoch: 117, Train Acc: 0.7333, Test Acc: 0.6708\n",
            "Epoch: 118, Train Acc: 0.7733, Test Acc: 0.6895\n",
            "Epoch: 119, Train Acc: 0.7667, Test Acc: 0.6906\n",
            "Epoch: 120, Train Acc: 0.7733, Test Acc: 0.6968\n",
            "Epoch: 121, Train Acc: 0.7533, Test Acc: 0.6916\n",
            "Epoch: 122, Train Acc: 0.7667, Test Acc: 0.6739\n",
            "Epoch: 123, Train Acc: 0.7667, Test Acc: 0.6864\n",
            "Epoch: 124, Train Acc: 0.7600, Test Acc: 0.6885\n",
            "Epoch: 125, Train Acc: 0.7600, Test Acc: 0.6947\n",
            "Epoch: 126, Train Acc: 0.7800, Test Acc: 0.6989\n",
            "Epoch: 127, Train Acc: 0.7867, Test Acc: 0.6999\n",
            "Epoch: 128, Train Acc: 0.7667, Test Acc: 0.6916\n",
            "Epoch: 129, Train Acc: 0.7733, Test Acc: 0.6822\n",
            "Epoch: 130, Train Acc: 0.7867, Test Acc: 0.6885\n",
            "Epoch: 131, Train Acc: 0.7800, Test Acc: 0.6957\n",
            "Epoch: 132, Train Acc: 0.7600, Test Acc: 0.6854\n",
            "Epoch: 133, Train Acc: 0.7533, Test Acc: 0.6812\n",
            "Epoch: 134, Train Acc: 0.7667, Test Acc: 0.6812\n",
            "Epoch: 135, Train Acc: 0.7733, Test Acc: 0.6978\n",
            "Epoch: 136, Train Acc: 0.7800, Test Acc: 0.6947\n",
            "Epoch: 137, Train Acc: 0.7467, Test Acc: 0.6864\n",
            "Epoch: 138, Train Acc: 0.7333, Test Acc: 0.6739\n",
            "Epoch: 139, Train Acc: 0.7467, Test Acc: 0.6822\n",
            "Epoch: 140, Train Acc: 0.7667, Test Acc: 0.6698\n",
            "Epoch: 141, Train Acc: 0.7800, Test Acc: 0.7155\n",
            "Epoch: 142, Train Acc: 0.7667, Test Acc: 0.6916\n",
            "Epoch: 143, Train Acc: 0.7467, Test Acc: 0.6771\n",
            "Epoch: 144, Train Acc: 0.7400, Test Acc: 0.6750\n",
            "Epoch: 145, Train Acc: 0.7667, Test Acc: 0.6677\n",
            "Epoch: 146, Train Acc: 0.7600, Test Acc: 0.6937\n",
            "Epoch: 147, Train Acc: 0.7733, Test Acc: 0.6771\n",
            "Epoch: 148, Train Acc: 0.7667, Test Acc: 0.6937\n",
            "Epoch: 149, Train Acc: 0.7467, Test Acc: 0.6854\n",
            "Epoch: 150, Train Acc: 0.7467, Test Acc: 0.6677\n",
            "Epoch: 151, Train Acc: 0.7400, Test Acc: 0.6812\n",
            "Epoch: 152, Train Acc: 0.7733, Test Acc: 0.6895\n",
            "Epoch: 153, Train Acc: 0.7533, Test Acc: 0.6989\n",
            "Epoch: 154, Train Acc: 0.7667, Test Acc: 0.6999\n",
            "Epoch: 155, Train Acc: 0.7867, Test Acc: 0.7103\n",
            "Epoch: 156, Train Acc: 0.7733, Test Acc: 0.6989\n",
            "Epoch: 157, Train Acc: 0.7400, Test Acc: 0.6625\n",
            "Epoch: 158, Train Acc: 0.7133, Test Acc: 0.6511\n",
            "Epoch: 159, Train Acc: 0.7467, Test Acc: 0.6822\n",
            "Epoch: 160, Train Acc: 0.7733, Test Acc: 0.6968\n",
            "Epoch: 161, Train Acc: 0.7800, Test Acc: 0.6968\n",
            "Epoch: 162, Train Acc: 0.7667, Test Acc: 0.6999\n",
            "Epoch: 163, Train Acc: 0.7400, Test Acc: 0.6791\n",
            "Epoch: 164, Train Acc: 0.7467, Test Acc: 0.6771\n",
            "Epoch: 165, Train Acc: 0.7600, Test Acc: 0.6978\n",
            "Epoch: 166, Train Acc: 0.7800, Test Acc: 0.7061\n",
            "Epoch: 167, Train Acc: 0.7600, Test Acc: 0.6854\n",
            "Epoch: 168, Train Acc: 0.7600, Test Acc: 0.6885\n",
            "Epoch: 169, Train Acc: 0.7667, Test Acc: 0.6864\n",
            "Epoch: 170, Train Acc: 0.7800, Test Acc: 0.6739\n",
            "Epoch: 171, Train Acc: 0.7867, Test Acc: 0.7040\n",
            "Epoch: 172, Train Acc: 0.7400, Test Acc: 0.6604\n",
            "Epoch: 173, Train Acc: 0.7067, Test Acc: 0.6449\n",
            "Epoch: 174, Train Acc: 0.7467, Test Acc: 0.6719\n",
            "Epoch: 175, Train Acc: 0.7733, Test Acc: 0.6781\n",
            "Epoch: 176, Train Acc: 0.7867, Test Acc: 0.6885\n",
            "Epoch: 177, Train Acc: 0.7867, Test Acc: 0.6968\n",
            "Epoch: 178, Train Acc: 0.7467, Test Acc: 0.6926\n",
            "Epoch: 179, Train Acc: 0.7600, Test Acc: 0.6937\n",
            "Epoch: 180, Train Acc: 0.7667, Test Acc: 0.6874\n",
            "Epoch: 181, Train Acc: 0.7733, Test Acc: 0.6812\n",
            "Epoch: 182, Train Acc: 0.7467, Test Acc: 0.6885\n",
            "Epoch: 183, Train Acc: 0.7667, Test Acc: 0.6874\n",
            "Epoch: 184, Train Acc: 0.7800, Test Acc: 0.6978\n",
            "Epoch: 185, Train Acc: 0.7600, Test Acc: 0.6926\n",
            "Epoch: 186, Train Acc: 0.7467, Test Acc: 0.6906\n",
            "Epoch: 187, Train Acc: 0.7600, Test Acc: 0.6822\n",
            "Epoch: 188, Train Acc: 0.7667, Test Acc: 0.7030\n",
            "Epoch: 189, Train Acc: 0.7800, Test Acc: 0.7103\n",
            "Epoch: 190, Train Acc: 0.7733, Test Acc: 0.6833\n",
            "Epoch: 191, Train Acc: 0.7467, Test Acc: 0.6916\n",
            "Epoch: 192, Train Acc: 0.7600, Test Acc: 0.6906\n",
            "Epoch: 193, Train Acc: 0.7733, Test Acc: 0.7040\n",
            "Epoch: 194, Train Acc: 0.7800, Test Acc: 0.6698\n",
            "Epoch: 195, Train Acc: 0.7600, Test Acc: 0.6781\n",
            "Epoch: 196, Train Acc: 0.7533, Test Acc: 0.6885\n",
            "Epoch: 197, Train Acc: 0.7667, Test Acc: 0.7009\n",
            "Epoch: 198, Train Acc: 0.7733, Test Acc: 0.6750\n",
            "Epoch: 199, Train Acc: 0.7800, Test Acc: 0.6636\n",
            "Epoch: 200, Train Acc: 0.7600, Test Acc: 0.6812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYv8FXPQOea2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}