{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PROTEINS_Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lqL-LQstoi0"
      },
      "source": [
        "# Graph Neural Network Classification on the PROTEINS Dataset\n",
        "\n",
        "For the first approach, I'm going to use [Spektral](https://graphneural.network/getting-started/) for Python to build my GCN layer and then perform our classification. \n",
        "\n",
        "Spektral is a library for Python for Graph Neural Networks, built on Tensorflow and Keras. \n",
        "\n",
        "For the second approach, I'm going to build a GCN from scratch, to experiment with Kipf & Welling's Graph Convolutional Network approach to embedding graphs into vector space. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_884Zbltkwo"
      },
      "source": [
        "# !pip install spektral"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw3cynAtwQOE",
        "outputId": "18ecf3fc-03c3-4913-e0c6-49cc5038ba89"
      },
      "source": [
        "# Reading in the PROTEINS dataset\n",
        "from spektral.datasets import TUDataset\n",
        "\n",
        "data = TUDataset('PROTEINS')\n",
        "data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully loaded PROTEINS.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TUDataset(n_graphs=1113)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AZCWahwwj7A"
      },
      "source": [
        "# Since we want to utilize the Spektral GCN layer, we want to follow the original paper for this method and perform some preprocessing:\n",
        "from spektral.transforms import GCNFilter\n",
        "\n",
        "data.apply(GCNFilter())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDNI3sTt5mJQ"
      },
      "source": [
        "# Split our train and test data. This just splits based on the first 80%/second 20% which isn't entirely ideal, so we'll shuffle the data first.\n",
        "import numpy as np\n",
        "\n",
        "np.random.shuffle(data)\n",
        "split = int(0.8 * len(data))\n",
        "data_train, data_test = data[:split], data[split:]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwvnn79Ewm7M"
      },
      "source": [
        "# Spektral is built on top of Keras, so we can use the Keras functional API to build a model that first embeds,\n",
        "# then sums the nodes together (global pooling), then classifies the result with a dense softmax layer\n",
        "\n",
        "# First, let's import the necessary layers:\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from spektral.layers import GCNConv, GlobalSumPool"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrTSQXkZyW78"
      },
      "source": [
        "# Now, we can use model subclassing to define our model:\n",
        "\n",
        "class ProteinsGNN(Model):\n",
        "  \n",
        "  def __init__(self, n_hidden, n_labels):\n",
        "    super().__init__()\n",
        "    # Define our GCN layer with our n_hidden layers\n",
        "    self.graph_conv = GCNConv(n_hidden)\n",
        "    # Define our global pooling layer\n",
        "    self.pool = GlobalSumPool()\n",
        "    # Define our dropout layer, initialize dropout freq. to .5 (50%)\n",
        "    self.dropout = Dropout(0.5)\n",
        "    # Define our Dense layer, with softmax activation function\n",
        "    self.dense = Dense(n_labels, 'softmax')\n",
        "\n",
        "  # Define class method to call model on input\n",
        "  def call(self, inputs):\n",
        "    out = self.graph_conv(inputs)\n",
        "    out = self.dropout(out)\n",
        "    out = self.pool(out)\n",
        "    out = self.dense(out)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CWW1urKzRrU"
      },
      "source": [
        "# Instantiate our model for training\n",
        "model = ProteinsGNN(32, data.n_labels)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG2YY6CMzf3I"
      },
      "source": [
        "# Compile model with our optimizer (adam) and loss function\n",
        "model.compile('adam', 'categorical_crossentropy')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw1hlui8zpYg"
      },
      "source": [
        "# Here's the trick - we can't just call Keras' fit() method on this model.\n",
        "# Instead, we have to use Loaders, which Spektral walks us through. Loaders create mini-batches by iterating over the graph\n",
        "# Since we're using Spektral for an experiment, for our first trial we'll use the recommended loader in the getting started tutorial\n",
        "\n",
        "# TODO: read up on modes and try other loaders later\n",
        "from spektral.data import BatchLoader\n",
        "\n",
        "loader = BatchLoader(data_train, batch_size=32)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmJ70FZy0Jgw",
        "outputId": "d16dc05a-076b-4d2b-d8fb-ce41d043ae3b"
      },
      "source": [
        "# Now we can train! We don't need to specify a batch size, since our loader is basically a generator\n",
        "# But we do need to specify the steps_per_epoch parameter\n",
        "\n",
        "model.fit(loader.load(), steps_per_epoch=loader.steps_per_epoch, epochs=10)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 9.9767\n",
            "Epoch 2/10\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 7.7281\n",
            "Epoch 3/10\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 6.2564\n",
            "Epoch 4/10\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 6.8929\n",
            "Epoch 5/10\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 6.8060\n",
            "Epoch 6/10\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 5.8402\n",
            "Epoch 7/10\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 5.2459\n",
            "Epoch 8/10\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 5.0838\n",
            "Epoch 9/10\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 5.3727\n",
            "Epoch 10/10\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 6.2093\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb280f7c2d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GOzdAb01Ei4"
      },
      "source": [
        "# To evaluate, let's instantiate another loader to test\n",
        "\n",
        "test_loader = BatchLoader(data_test, batch_size=32)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wZS1SEK6H4o",
        "outputId": "41622731-a5e0-40fc-f6a3-97fc984be6e0"
      },
      "source": [
        "# And feed it to our model by calling .load()\n",
        "\n",
        "loss = model.evaluate(loader.load(), steps=loader.steps_per_epoch)\n",
        "\n",
        "print('Test loss: {}'.format(loss))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28/28 [==============================] - 1s 15ms/step - loss: 3.6071\n",
            "Test loss: 3.6071038246154785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3nD1n0L6RaA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}