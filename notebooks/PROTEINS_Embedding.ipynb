{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PROTEINS_Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lqL-LQstoi0"
      },
      "source": [
        "# Graph Neural Network Classification on the PROTEINS Dataset\n",
        "\n",
        "For the first approach, I'm going to use [Spektral](https://graphneural.network/getting-started/) for Python to build my GCN layer and then perform our classification. \n",
        "\n",
        "Spektral is a library for Python for Graph Neural Networks, built on Tensorflow and Keras. \n",
        "\n",
        "For the second approach, I'm going to build a GCN from scratch, to experiment with Kipf & Welling's Graph Convolutional Network approach to embedding graphs into vector space. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_884Zbltkwo"
      },
      "source": [
        "# !pip install spektral"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw3cynAtwQOE",
        "outputId": "18ecf3fc-03c3-4913-e0c6-49cc5038ba89"
      },
      "source": [
        "# Reading in the PROTEINS dataset\n",
        "from spektral.datasets import TUDataset\n",
        "\n",
        "data = TUDataset('PROTEINS')\n",
        "data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully loaded PROTEINS.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TUDataset(n_graphs=1113)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AZCWahwwj7A"
      },
      "source": [
        "# Since we want to utilize the Spektral GCN layer, we want to follow the original paper for this method and perform some preprocessing:\n",
        "from spektral.transforms import GCNFilter\n",
        "\n",
        "data.apply(GCNFilter())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwvnn79Ewm7M"
      },
      "source": [
        "# Spektral is built on top of Keras, so we can use the Keras functional API to build a model that first embeds,\n",
        "# then sums the nodes together (global pooling), then classifies the result with a dense softmax layer\n",
        "\n",
        "# First, let's import the necessary layers:\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from spektral.layers import GCNConv, GlobalSumPool"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrTSQXkZyW78"
      },
      "source": [
        "# Now, we can use model subclassing to define our model:\n",
        "\n",
        "class ProteinsGNN(Model):\n",
        "  \n",
        "  def __init__(self, n_hidden, n_labels):\n",
        "    super().__init__()\n",
        "    # Define our GCN layer with our n_hidden layers\n",
        "    self.graph_conv = GCNConv(n_hidden)\n",
        "    # Define our global pooling layer\n",
        "    self.pool = GlobalSumPool()\n",
        "    # Define our dropout layer, initialize dropout freq. to .5 (50%)\n",
        "    self.dropout = Dropout(0.5)\n",
        "    # Define our Dense layer, with softmax activation function\n",
        "    self.dense = Dense(n_labels, 'softmax')\n",
        "\n",
        "  # Define class method to call model on input\n",
        "  def call(self, inputs):\n",
        "    out = self.graph_conv(inputs)\n",
        "    out = self.dropout(out)\n",
        "    out = self.pool(out)\n",
        "    out = self.dense(out)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CWW1urKzRrU"
      },
      "source": [
        "# Instantiate our model for training\n",
        "model = ProteinsGNN(32, data.n_labels)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG2YY6CMzf3I"
      },
      "source": [
        "# Compile model with our optimizer (adam) and loss function\n",
        "model.compile('adam', 'categorical_crossentropy')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw1hlui8zpYg"
      },
      "source": [
        "# Here's the trick - we can't just call Keras' fit() method on this model.\n",
        "# Instead, we have to use Loaders, which Spektral walks us through. Loaders create mini-batches by iterating over the graph\n",
        "# Since we're using Spektral for an experiment, for our first trial we'll use the recommended loader in the getting started tutorial\n",
        "\n",
        "# TODO: read up on modes and try other loaders later\n",
        "from spektral.data import BatchLoader\n",
        "\n",
        "loader = BatchLoader(data, batch_size=32)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmJ70FZy0Jgw",
        "outputId": "45b9b755-8f46-4414-a811-b76f41df7255"
      },
      "source": [
        "# Now we can train! We don't need to specify a batch size, since our loader is basically a generator\n",
        "# But we do need to specify the steps_per_epoch parameter\n",
        "\n",
        "model.fit(loader.load(), steps_per_epoch=loader.steps_per_epoch, epochs=10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "35/35 [==============================] - 3s 28ms/step - loss: 95.9733\n",
            "Epoch 2/10\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 14.3282\n",
            "Epoch 3/10\n",
            "35/35 [==============================] - 1s 21ms/step - loss: 6.8267\n",
            "Epoch 4/10\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 7.8008\n",
            "Epoch 5/10\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 6.6979\n",
            "Epoch 6/10\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 6.1409\n",
            "Epoch 7/10\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 6.2961\n",
            "Epoch 8/10\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 6.1376\n",
            "Epoch 9/10\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 6.6149\n",
            "Epoch 10/10\n",
            "35/35 [==============================] - 1s 26ms/step - loss: 5.6460\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb288297810>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GOzdAb01Ei4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}